{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'version:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SPARK_HOME:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/usr/local/Cellar/apache-spark/2.0.2/libexec'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pyspark # does not seem necessary\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row # for RDDs\n",
    "\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "conf = sc.getConf()\n",
    "if conf.get('spark.master') == 'yarn':\n",
    "    print(\"cloud\")\n",
    "    root_path =\"gs://josh-spark-jupyter-resources\"\n",
    "else:\n",
    "    print(\"local\")\n",
    "    root_path = os.path.split(os.getcwd())[0] + \"/resources\"\n",
    "\n",
    "json_path = root_path + \"/people.json\"\n",
    "txt_path = root_path + \"/people.txt\"\n",
    "display(type(sc))\n",
    "display(type(spark))\n",
    "display('version:', spark.version)\n",
    "display('SPARK_HOME:', os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[5, 9, 10]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick: 1\n",
      "jumps: 1\n",
      "cat: 1\n",
      "Tyger: 2\n",
      "night: 1\n",
      "lazy: 1\n",
      "brown: 1\n",
      "forests: 1\n",
      "hat: 1\n",
      "in: 2\n",
      "the: 6\n",
      "burning: 1\n",
      "of: 1\n",
      "fox: 1\n",
      "bright: 1\n",
      "dog: 1\n",
      "over: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RDD from list\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "display(type(distData))\n",
    "\n",
    "# Saving and Loading SequenceFiles\n",
    "import random\n",
    "tmp_file = \"/tmp/blah\" + str(random.randint(1,10000))\n",
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x ))\n",
    "rdd.saveAsSequenceFile(tmp_file)\n",
    "display(sorted(sc.sequenceFile(tmp_file).collect()))\n",
    "\n",
    "\n",
    "txt_path2 = root_path + \"/words.txt\"\n",
    "\n",
    "# mapreduce\n",
    "lines = sc.textFile(txt_path2)\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "display(totalLength)\n",
    "\n",
    "# passing functions\n",
    "def line_word_count(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "display(sc.textFile(txt_path2).map(line_word_count).collect())\n",
    "\n",
    "# wordcount reduceByKey\n",
    "from operator import add\n",
    "lines = spark.read.text(txt_path2).rdd.map(lambda r: r[0])\n",
    "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(add)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "    \n",
    "# shared variables\n",
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "display(broadcastVar.value)\n",
    "accum = sc.accumulator(0)\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "display(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(json_path)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untyped Dataset Operations (aka DataFrame Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert from Spark DF to Pandas DF\n",
    "pandas_df = df.toPandas()\n",
    "display(type(pandas_df))\n",
    "# convert from Pandas DF to Spark DF\n",
    "df2 = spark.createDataFrame(pandas_df)\n",
    "display(type(df2))\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(txt_path)\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "display(people)\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a simple DataFrame, stored into a partition directory\n",
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                  .map(lambda i: Row(single=i, double=i ** 2)))\n",
    "squaresDF.write.parquet(\"/tmp/test_table/key=1\")\n",
    "\n",
    "# Create another DataFrame in a new partition directory,\n",
    "# adding a new column and dropping an existing column\n",
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "cubesDF.write.parquet(\"/tmp/test_table/key=2\")\n",
    "\n",
    "# Read the partitioned table\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"/tmp/test_table\")\n",
    "mergedDF.printSchema()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gcloud]",
   "language": "python",
   "name": "conda-env-gcloud-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
